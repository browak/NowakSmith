\documentclass{article}
\newcommand{\pkg}[1]{{\fontseries{b}\selectfont #1}} 
\usepackage{listings}
\begin{document}

<<setup, echo=FALSE , include=FALSE , tidy=T>>=
library(knitr)
library(texreg)
opts_chunk$set(size = 'footnotesize')
options(width = 80 , tidy=T ,str = strOptions(strict.width = "wrap") ,warn=-1)
@


\section{Installing \pkg{hedonicText}}
Using \pkg{devtools}, install \pkg{hedonicText}
<<>>=
library(devtools)
install_github("browak/NowakSmith/hedonicText" , quiet=T)
library(hedonicText)
@
\noindent \pkg{hedonicText} cannot be installed using CRAN.

\section{Data}
\pkg{hedonicText} comes with a data set of 24,446 property transactions with sale price, square footage, and remarks.  The remarks are text descriptions of the property created by the listing agent.  The data set can be loaded using
<<tidy=TRUE>>=
data(listings)
listings <- listings
str(listings)
@

\section{Functions}
The remarks in the data section are all capitalized, include letters, numbers, and non alpha-numeric characters.  \pkg{hedonicText} includes a basic function, \pkg{cleanText}, to clean the text.  Of course, you can always clean the text yourself.  

<<>>=
cleanRemarks <- cleanText(listings$remarks)
str(cleanRemarks)
@

By default, \pkg{cleanText} will convert all text to lowercase, remove numbers, remove punctuation, remove stop words, and remove single letters.  The list of stop words comes from the list of stopwords in the \pkg{tm} package.

You can also identify the most frequent n-grams in the data.  These n-grams can be used to tokenize the text.  This is done using the \pkg{flexGramCount} function.  The most frequent unigrams are found using

<<>>=
unigramCount <- flexGramCount(cleanRemarks , maxN=1 , minN=1)
head(unigramCount)
@

\noindent Bigrams contain more detailed information but occur less frequently
<<>>=
bigramCount <- flexGramCount(cleanRemarks , maxN=2 , minN=2)
head(bigramCount)
@

\noindent Trigrams contain even more information but occur even less frequently
<<>>=
trigramCount <- flexGramCount(cleanRemarks , maxN=3 , minN=3)
head(trigramCount)
@

You can also identify a blend of n-grams for various n using
<<>>=
flexgramCount <- flexGramCount(cleanRemarks , maxN=5 , minN=2)
head(subset(flexgramCount , N==2) , n=3)
head(subset(flexgramCount , N==3) , n=3)
head(subset(flexgramCount , N==4) , n=4)
@

\section{Hedonic Models}
Using a given set of tokens, you can estimate a hedonic pricing model. First, you create a matrix of indicator variables for each of the tokens
<<>>=
tokenList <- unigramCount$ngrams
M <- tokenMatrix(cleanRemarks , tokenList)
str(M)
@

The tokens matrix can be used with additional regressors in a hedonic model.  The additional regressors are not penalized in the penalized regression.  The model is then estimated using a LASSO with a cross-validated penalty term.
<<>>=
y <- log(listings$price)
X <- cbind(listings$sqft , listings$sqft**2)
unigramFit <- hedonicWithText(y , X , M)
@

The coefficients for the tokens in the penalized regression can be extracted using
<<>>=
bFit <- coef(unigramFit , s='lambda.min')
@

\pkg{tokenTable} will transform coefficients into a data frame for presentation

<<>>=
tokenDataFrame <- tokenTable(bFit)
head(tokenDataFrame , n=10)
@

\end{document}